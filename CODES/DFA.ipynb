{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token value: AI\n",
      "Token value: redirects\n",
      "Token value: here\n",
      "Token value: For\n",
      "Token value: other\n",
      "Token value: uses\n",
      "Token value: see\n",
      "Token value: AI\n",
      "Token value: disambiguation\n",
      "Token value: Artificial\n",
      "Token value: intelligence\n",
      "Token value: disambiguation\n",
      "Token value: and\n",
      "Token value: Intelligent\n",
      "Token value: agent\n",
      "Token value: Part\n",
      "Token value: of\n",
      "Token value: a\n",
      "Token value: series\n",
      "Token value: on\n",
      "Token value: Artificial\n",
      "Token value: intelligence\n",
      "Token value: Anatomy1751201\n",
      "Token value: \n",
      "Major\n",
      "Token value: goals\n",
      "Token value: Approaches\n",
      "Token value: Philosophy\n",
      "Token value: History\n",
      "Token value: Technology\n",
      "Token value: Glossary\n",
      "Token value: vte\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# Set of all characters that can appear in the input\n",
    "alphabet = set(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789_\")\n",
    "\n",
    "# Set of keywords\n",
    "keywords = set(\"if else while int return void\")\n",
    "\n",
    "# Set of operators\n",
    "operators = set(\"+ - * /\")\n",
    "\n",
    "# Set of separators\n",
    "separators = set(\"( ) { } [ ] , ;\")\n",
    "\n",
    "# Set of whitespaces\n",
    "whitespaces = set(\" \\t\\n\")\n",
    "\n",
    "# DFA states\n",
    "START = 0\n",
    "IDENTIFIER = 1\n",
    "INTEGER = 2\n",
    "OPERATOR = 3\n",
    "SEPARATOR = 4\n",
    "END = 5\n",
    "def lex(input_str):\n",
    "    \"\"\"Perform lexical analysis on the input string\"\"\"\n",
    "    tokens = []\n",
    "    i = 0\n",
    "    while i < len(input_str):\n",
    "        # Initialize the DFA state to start state\n",
    "        state = START\n",
    "        j = i\n",
    "        while True:\n",
    "            # Get the current character\n",
    "            c = input_str[j]\n",
    "\n",
    "            if state == START:\n",
    "                if c in alphabet:\n",
    "                    state = IDENTIFIER\n",
    "                elif c in \"0123456789\":\n",
    "                    state = INTEGER\n",
    "                elif c in operators:\n",
    "                    state =OPERATOR\n",
    "                elif c in whitespaces:\n",
    "                    state = START\n",
    "                else:\n",
    "                    print(f\"Invalid character: {c}\")\n",
    "                    sys.exit(1)\n",
    "            elif state == IDENTIFIER:\n",
    "                if c in alphabet:\n",
    "                    pass\n",
    "                else:\n",
    "                    state = END\n",
    "                    j -= 1\n",
    "            elif state == INTEGER:\n",
    "                if c in \"0123456789\":\n",
    "                    pass\n",
    "                else:\n",
    "                    state = END\n",
    "                    j -= 1\n",
    "            elif state == OPERATOR:\n",
    "                state = END\n",
    "                j -= 1\n",
    "            elif state == SEPARATOR:\n",
    "                state = END\n",
    "                j -= 1\n",
    "            elif state == END:\n",
    "                # Tokenization complete\n",
    "                break\n",
    "            j += 1\n",
    "\n",
    "        # Determine the token type based on the DFA state\n",
    "        token_value = input_str[i:j]\n",
    "        if state == IDENTIFIER:\n",
    "            token_type = \"IDENTIFIER\"\n",
    "            if token_value in keywords:\n",
    "                token_type = \"KEYWORD\"\n",
    "        elif state == INTEGER:\n",
    "            token_type = \"INTEGER\"\n",
    "        elif state == OPERATOR:\n",
    "            token_type = \"OPERATOR\"\n",
    "        elif state == SEPARATOR:\n",
    "            token_type = \"SEPARATOR\"\n",
    "        else:\n",
    "            token_type=\"UNKNOWN\"\n",
    "\n",
    "        # Add the token to the list of tokens\n",
    "        tokens.append((token_type, token_value))\n",
    "        # Move to the next character\n",
    "        i = j + 1\n",
    "\n",
    "    return tokens\n",
    "def main():\n",
    "    # Read the input from a file\n",
    "    with open(\"C:/Users/Aravind Vadlapudi/OneDrive - Amrita Vishwa Vidyapeetham/Desktop/Clg Documents/Semester-5/FLA Project/input.txt\", \"r\") as f:\n",
    "        input_str = f.read()\n",
    "\n",
    "    # Perform lexical analysis\n",
    "    tokens = lex(input_str)\n",
    "\n",
    "    # Print the results for each token\n",
    "    for token_type, token_value in tokens:\n",
    "        #print(f\"Token type: {token_type}\")\n",
    "        print(f\"Token value: {token_value}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
